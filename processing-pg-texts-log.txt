##These are some linux commands to help data scientists prepare project gutenberg english ebooks for analysis

download
wget -H -w 2 -m http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en 
##where -h is recursive and "-w 2" is a two second delay between requests

unzip
while [ "`find . -type f -name '*.zip' | wc -l`" -gt 0 ]; do find -type f -name "*.zip" -exec unzip -- '{}' \; -exec rm -- '{}' \;; done
##command cycles through subdirectories unzipping, and placing unzipped file in parent directory where script was intiated

##remove first seven files from database; these lack the PG TOS following ebook; impacted by later trimming
rm 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt

triming text from files conditionally
##All project gutenberg ebooks contain a terms of service contract at the end of the book; great for lawyers, not
##great for data science tasks. You will need to remove the terms of service, and potentially the beginning too.
sed -i'.bak' '/END OF THIS PROJECT GUTENBERG EBOOK/,$d' *.txt
##after confirming sed is removing correctly, remove '.bak' but leaving the -i, which edits file inplace, otherwise a backup of each
##file will be made.
##potential additions to sed include -u -n options

##after trimming the files of the trailing terms of service you may also need to check for other text you need to remove and see how many files have said text
grep -rnw '/home/marvin/projects/archive' -e '*** START OF THIS PROJECT GUTENBERG EBOOK' | wc -l
##this will count the total ebooks with the single quoted text '*** START OF'...etc


other helpful commands
wc -l < 10002.txt 10002.txt.bak
##to count the lines in the files to see if the licensing data tail has been cleaned

diff 10002.txt 10002.txt.bak 
##will compare the two files and output the differences

du -sh /directory
##this will give you a rough number of storage used by a specific directory, containing your gutenberrg files

##archiving
##you will be making lots of edits to lots of files; this would be a good time to start talking about version
##control. I would recommend getting an external hard drive or remote storage option (assuming you have a super fast
##internet connect) otherwise stick with physical drive backup. After you've unzipped your files and started trim,
##you will need to store the .txt.bak files somewhere, and potentially make copies of the trimmed *.txt files. The files
##expand from 14GB zipped to around 40GB unzipped.

rsync -r --include='*.txt' --exclude='*' . ../dirFullOfEbooks
##run this in the source directory and replace ../dirFullOfEbooks with the desired storage directory or device
##mv and cp commands are also helpful but the cp command has a max number of files it can run against and mv does
##not work since it does not leave a copy of the file behind. Rysnc is the best command for making copies of 80,000
##ebooks txt files

##depending on what tool you pick (NLP or ML tools) you may then have to zip the files again to upload to an S3 bucket
##or a Floyd-hub cloud storage. In which case you may want to keep the original .txt file and zip a copy to another directory
zip -r  /path/to/save/destination_folder.zip /path/to/folder


##you may also need to download all of the project gutenberg metadata
wget http://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.zip && unzip rdf-files.tar.zip && tar -xvf *.tar


