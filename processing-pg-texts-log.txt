##These are some linux commands to help data scientists prepare project gutenberg english ebooks for analysis

download
wget -H -w 2 -m http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en 
##where -h is recursive and "-w 2" is a two second delay between requests

unzip
while [ "`find . -type f -name '*.zip' | wc -l`" -gt 0 ]; do find -type f -name "*.zip" -exec unzip -- '{}' \; -exec rm -- '{}' \;; done
##command cycles through subdirectories unzipping, and placing unzipped file in parent directory where script was intiated

##remove first seven files from database; these lack the PG TOS following ebook; impacted by later trimming
rm 1.txt 2.txt 3.txt 4.txt 5.txt 6.txt 7.txt

trim/cut
##All project gutenberg ebooks contain a terms of service contract at the end of the book; great for lawyers, not
##great for data science tasks. You will need to remove the terms of service, and potentially the beginning too.
sed -i'.bak' '/END OF THIS PROJECT GUTENBERG EBOOK/,$d' *.txt
##after confirming sed is removing correctly, remove -i'.bak' which edits file inplace, otherwise a backup of each
##file will be made.
##potential additions to sed include -u -n options

other helpful commands
wc -l < 10002.txt 10002.txt.bak
##to count the lines in the files to see if the licensing data tail has been cleaned

diff 10002.txt 10002.txt.bak 
##will compare the two files and output the differences

du -sh /directory
##this will give you a rough number of storage used by a specific directory, containing your gutenberrg files

##archiving
##you will be making lots of edits to lots of files; this would be a good time to start talking about version
##control. I would recommend getting an external hard drive or remote storage option (assuming you have a super fast
##internet connect) otherwise stick with physical drive backup. After you've unzipped your files and started trim,
##you will need to store the .txt.bak files somewhere, and potentially make copies of the trimmed *.txt files. The files
##expand from 14GB zipped to around 40GB unzipped.

rsync -r --include='*.txt' --exclude='*' . ../dirFullOfEbooks
##run this in the source directory and replace ../dirFullOfEbooks with the desired storage directory or device
##mv and cp commands are also helpful but the cp command has a max number of files it can run against and mv does
##not work since it does not leave a copy of the file behind. Rysnc is the best command for making copies of 80,000
##ebooks txt files

##depending on what tool you pick (NLP or ML tools) you may then have to zip the files again to upload to an S3 bucket
##or a Floyd-hub cloud storage. In which case you may want to keep the original .txt file and zip a copy to another directory
zip -r  /path/to/save/destination_folder.zip /path/to/folder


##you may also need to download all of the project gutenberg metadata
wget http://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.zip
